---
title: "Live Assignment 10-11"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(rmdformats)
library(tidyverse)
library(caret)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


## Background  

Brewmeisters in Colorado and Texas have teamed up to analyze the relationship between ABV and IBU in each of their states.  Use the data sets from the project to help them in their analysis.  There three main questions of interest are 1) Is there a significant linear relationship between ABV (response) and IBU (explanatory), 2) Is this relationship different between beers in Colorado and Texas and 3) Is there a significant quadratic component in this relationship for either Colorado or Texas or both?  

## KNN Regression versus Linear Regression  
## A. Clean an prepare the data: (Same as Unit 9 Assignment (nothing new to do).   
###  1. Create column for brewery ID that is common to both datasets similar to what you did in the project. So we can merge!  
###  2. Merge the beer and brewery data into a single dataframe.  
###  3. Clean the State Column … get rid of extraneous white space.   
###  4. Create One Dataset that has only Colorado and Texas beers and no IBU NAs … name it “beerCOTX”  
###  5. Order beerCOTX by IBU (ascending) ... this will be important later in graphing  
  
```{r}
beers <- read_csv("Beers.csv")
beers %>% rename(Beer = "Name", Brew_ID = "Brewery_id") -> beers

breweries <- read_csv("Breweries.csv")
breweries %>% rename(Brewery = "Name") -> breweries
combined <- beers %>% left_join(breweries, by = "Brew_ID")
combined$State <- str_trim(combined$State)
beerCOTX <- combined %>% filter(State %in% c("CO","TX"), !is.na(IBU))
beerCOTX %>% arrange(IBU) -> beerCOTX
```
## B. Compare two competing models: External Cross Validation  
###  8. For this assignment we will concentrate only on the Texas data!  Create a training and test set from the data (60%/40% split respectively).  Print a summary of each new data frame… there should be two: TrainingTX, TestTX.  

```{r}
set.seed(314159)
tx.data <- beerCOTX[beerCOTX$State == "TX",]
TrainingTX <- sample_frac(tx.data, 0.6)
TestTX <- anti_join(tx.data,TrainingTX, by = "Beer_ID")
```
  
###  9. Using the training dat, fit a KNN regression model to predict ABV from IBU.  You should use the knnreg function in the caret package.  Fit two separate models: one with k = 3 and one with k = 5.  (This is 2 models total.)  

```{r}

par(mfrow = c(1,2))

fit3 <- knnreg(ABV~IBU,data=TrainingTX, k=3)
plot(TestTX$ABV, predict(fit3, TestTX), ylim = c(0.04,0.10),xlim = c(0.04,0.10))

fit5 <- knnreg(ABV~IBU,data=TrainingTX, k=5)
plot(TestTX$ABV, predict(fit5, TestTX), ylim = c(0.04,0.10),xlim = c(0.04,0.10))
```
  
###  10. Use the ASE loss function and external cross validation to provide evidence as to which model (k = 3 or k = 5) is more appropriate.  Remember your answer should be supported with why you feel a certain model is appropriate.  Your analysis should include the average squared error (ASE) for each model from the test set.  Your analysis should also include a clear discussion, using the ASEs, as to which model you feel is more appropriate.  
ASE=  (∑▒(y ̃_i-y_i )^2 )/n  
Here y ̃_i is the predicted ABV for the ith beer, y_iis the actual ABV of the ith beer and n is the sample size.   
```{r}
MSE <- function(pred,obs) {
  mean((pred-obs)^2)
}

MSE(predict(fit3, TestTX), TestTX$ABV)
MSE(predict(fit5, TestTX), TestTX$ABV)
```
The k=5 model has a lower MSE, so it has a better fit.

###  11.  Now use the ASE loss function and external cross validation to provide evidence as to which model (the linear regression model from last week or the “best” KNN regression model from this week (from question 10)) is more appropriate. 
  
```{r}
lm.tx.fit <- lm(ABV~IBU,TrainingTX)
MSE(predict(lm.tx.fit, TestTX), TestTX$ABV)
MSE(predict(fit5, TestTX), TestTX$ABV)
```
The linear model has a slightly better fit here. That said, my MSE from last week on the same model was 6.979604e-05. Why did it change? The train/test split has a different seed. I don't consider the difference to be huge, but it would change our conclusion. It probably just means either model is acceptable in this situation.

###  12.  Use your “best” KNN regression model to predict the ABV for an IBU of 150, 170 and 190.  What issue do you see with using KNN to extrapolate?  
```{r}
values = data.frame(IBU = c(150, 170, 190))
predict(fit5,values)
```
  The KNN prediction is not continuous, so different values of IBU can predict the same ABV value, similar to tree methods.
  
## KNN Classification  
We would like to be able to use ABV and IBU to classify beers between 3 styles: American IPA and American Pale Ale.  
###  13.  Filter the beerCOTX dataframe for only beers that are from Texas and are American IPA and American Pale Ale.  
```{r}
tx.data.styles <- tx.data %>% filter(Style %in% c("American IPA", "American Pale Ale (APA)")) %>% select(IBU, ABV, Style,Beer_ID)
```
  
###  14.  Divide this filtered data set into a training and test set (60/40, training / test split).  
  
```{r}
set.seed(314159)
style.train.TX <- sample_frac(tx.data.styles, 0.6)
style.test.TX <- anti_join(tx.data.styles,TrainingTX, by = "Beer_ID")

train.x <- style.train.TX %>% select(IBU, ABV)
test.x <- style.test.TX %>% select(IBU, ABV)
```
  
###  15. Use the class package’s knn function to build an KNN classifier with k = 3 that will use ABV and IBU as features (explanatory variables) to classify Texas beers as American IPA or American Pale Ale using the Training data.  Use your test set to create a confusion table to estimate the accuracy, sensitivity and specificity of the model.  
```{r}
library(class)
knn3.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=3, prob=F)

caret::confusionMatrix(table(knn3.pred,style.test.TX$Style))
```
  The accuracy is 100$, the Sensitivity is 100% and the Specificity is 100%. Strangely accurate.
  
###  16.  Using the same process as in the last question, find the accuracy, sensitivity and specificity of a KNN model with k = 5.  Which is “better”?  Why? 
  
```{r}
knn4.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=4, prob=F)
caret::confusionMatrix(table(knn4.pred,style.test.TX$Style))

knn5.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=5, prob=F)
caret::confusionMatrix(table(knn5.pred,style.test.TX$Style))

knn6.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=6, prob=F)
caret::confusionMatrix(table(knn6.pred,style.test.TX$Style))

knn2.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=2, prob=F)
caret::confusionMatrix(table(knn2.pred,style.test.TX$Style))

knn1.pred  <- knn(train = train.x, test = test.x, cl = style.train.TX$Style, k=1, prob=F)
caret::confusionMatrix(table(knn1.pred,style.test.TX$Style))
```
I would argue that the K=2 model is best. It maintains the maximum accuracy with the fewest clusters. Too few or too many results in a drop from 100% accuracy. I think the low sample size and the specific test/train split is probably contributing to the abnormally high accuracy.

## BONUS (5 pts total)  
We did not have a lot data to build and test this classifier.  Check out the class package’s knn.cv function that will perform leave-one-out cross validation.  What is leave-one-out CV (2pts)?  Get the accuracy metric for from this function for both the k = 3 and k = 5 KNN classifiers (2pts).  Which model is suggested by the leave-one-out CV method (1pt)?    

### What is leave-one-out CV (2pts)?

Leave-one-out cross validation is k-fold cross validation where k = n, and n is the number of data points in the data set. You train the data on n-1 data points, then predict the remaining observation.  

### Get the accuracy metric for from this function for both the k = 3 and k = 5 KNN classifiers (2pts)
```{r}
cv.train <- tx.data.styles %>% select(IBU, ABV)
cv.truth <- as.factor(tx.data.styles$Style)
cv.knn3 <- knn.cv(train = cv.train, cl = cv.truth, k=3, prob=F)
caret::confusionMatrix(table(cv.knn3,cv.truth))

cv.knn5 <- knn.cv(train = cv.train, cl = cv.truth, k=5, prob=F)
caret::confusionMatrix(table(cv.knn5,cv.truth))

cv.knn2 <- knn.cv(train = cv.train, cl = cv.truth, k=2, prob=F)
caret::confusionMatrix(table(cv.knn2,cv.truth))
```


### Which model is suggested by the leave-one-out CV method (1pt)?   

  K=3 and K=5 have the same accuracy, so I would prefer the k=3 model for simplicity. K=2 results in lower accuracy. K=7 had the same as K=3.


## Unit 11 Questions

###  1. Use the most updated code that is zipped with this.  It fixes the grep problem by pasting a string with bracketed regular expression.   I think someone mentioned this in class.  Good call!  

I copied the code over to this document to make sure it was easy to view. The "NYT" chunk imports the data. The Segmentation chunk sets the categories and does train / test split. The "Headline" chunk does the accuracy for the headlines. The snippet chunk is the same as the headline chunk except now it calculates for snippets.  

```{r NYT, cache=TRUE}
######################

# Loading the Data from the NYT API

# We will load from "Data+Science" and "Trump" searches.

# Data+Science Search
# term <- "Data+Science" # Need to use + to string together separate words
# begin_date <- "20180901"
# end_date <- "20190502"

# Trump Search
# term <- "Trump" # Need to use + to string together separate words
# begin_date <- "20190415"
# end_date <- "20190502"

######################

#NYT Example

library(tm) #text mining library provides the stopwords() function
library(jsonlite)
library(tidyverse)
library(keyring) #protecting my API key

NYTIMES_KEY = key_get("NYTAPIKEY") #Your Key Here … get from NTY API website

# Let's set some parameters
term <- "Central+Park+Jogger" # Need to use + to string together separate words
begin_date <- "19890101"
end_date <- "19991231"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,"&end_date=",end_date,
                  "&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

baseurl

initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages

pages <- list()
for(i in 0:maxPages){
  nytSearch <- jsonlite::fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(7) 
}

allNYTSearch <- rbind_pages(pages)
#saveRDS(allNYTSearch,"allNYTSearch.rds")
```
```{r Segment}
#allNYTSearch<- loadRDS("allNYTSearch.rds")
#Segmentation

# Visualize coverage by section
allNYTSearch %>% 
  group_by(response.docs.type_of_material) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()

#Make another column of News versus Other ... The labels

allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
#There is an NA in NewsOrOther

# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>% 
  group_by(NewsOrOther) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()



#Train and Test Split 50%/50%

set.seed(2)
trainInd = sample(seq(1,dim(allNYTSearch)[1],1),round(.5*dim(allNYTSearch)[1]))
allNYTSearchTrain = allNYTSearch[trainInd,]
allNYTSearchTest = allNYTSearch[-trainInd,]
```

###  2. Use the “snippet” instead of the headline.  

See the snippet chunk in Q4  

###  3. Look at data from 1989 to 1999  

This is part of the NYT chunk aboave 

###  4. To provide external cross validation (50%-50%).  Create a training and test set from the total number of articles.  Train the classifier on the training set and create your confusion matrix from the test set. Make sure and provide the confusion matrix.  

```{r Snippet}
#This function returns P(News | Keyword) 
#P(News|KW) = P(KW|News)* P(News) / P(KW)
Pnews_word = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
  trainingSet$response.docs.snippet = unlist(str_replace_all(trainingSet$response.docs.snippet,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
  
  #print(key_word)
  NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
  OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
  
  pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
  pOther = 1 - pNews
  
  pKWGivenNews = (length(str_which(NewsGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
  pKWGivenOther = (length(str_which(OtherGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
  
  pKW = length(str_which(trainingSet$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
  
  pNewsGivenKW = pKWGivenNews*pNews/pKW
  pOtherGivenKW = pKWGivenOther*pOther/pKW
  
  return(pNewsGivenKW)
}

theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;


for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
  
  articleScoreNews = 1; 
  articleScoreOther = 1;

#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.  
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.snippet,"[^[:alnum:] ]", ""), boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.  

# stopwords() #from package tm
wordsToTakeOut = stopwords()

# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b") 
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut

importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]

#importantWords

  for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
  {
    articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
    articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
  }
  theScoreHolderNews[i] = articleScoreNews
  theScoreHolderOther[i] = articleScoreOther
}

# Classify the aricle as News or Other based on a given piece of information from the article.
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")

#Confusion Matrix
table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther) #Actual in Columns
caret::confusionMatrix(table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther))
```  

###  6. Provide accuracy, sensitivity and specificity from the confusion matrix.  You may consider News to be the positive.  

The accuracy is 0.6595%, the sensitivy is 0.85567 and the specificity is 0.05556

###  7. Use your statistics from the last two questions to assess whether the headline or the snippet makes for a better classifier.   

The accuracy for headlines is 0.5428%, the sensitivity is 0.5335, the specificity is 0.5714  (code below)
The accuracy for snippets is 0.6595%, the sensitivy is 0.85567, and the specificity is 0.05556

Snippets outperform in the first two metrics, by a large margin, and underperform in the 3rd metric by a small amount. I would use the snippet model

```{r Headline}

#This function returns P(News | Keyword) 
#P(News|KW) = P(KW|News)* P(News) / P(KW)
Pnews_word = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
  trainingSet$response.docs.headline.main = unlist(str_replace_all(trainingSet$response.docs.headline.main,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
  
  #print(key_word)
  NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
  OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
  
  pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
  pOther = 1 - pNews
  
  pKWGivenNews = (length(str_which(NewsGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
  pKWGivenOther = (length(str_which(OtherGroup$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
  
  pKW = length(str_which(trainingSet$response.docs.headline.main,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
  
  pNewsGivenKW = pKWGivenNews*pNews/pKW
  pOtherGivenKW = pKWGivenOther*pOther/pKW
  
  return(pNewsGivenKW)
}

theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;


for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
  
  articleScoreNews = 1; 
  articleScoreOther = 1;

#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.  
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.headline.main,"[^[:alnum:] ]", ""), boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.  

# stopwords() #from package tm
wordsToTakeOut = stopwords()

# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b") 
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut

importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]

#importantWords

  for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
  {
    articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
    articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
  }
  theScoreHolderNews[i] = articleScoreNews
  theScoreHolderOther[i] = articleScoreOther
}

# Classify the aricle as News or Other based on a given piece of information from the article.
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")

#Confusion Matrix
table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther) #Actual in Columns
caret::confusionMatrix(table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther))

```


